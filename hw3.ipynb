{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning homework3\n",
    "\n",
    "Huibo Zhao hz2480\n",
    "Lingjie Xu lx2222"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform General Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Lasso, Ridge\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract those files that contain our training data\n",
    "path = os.getcwd()\n",
    "files = os.listdir(path)\n",
    "xlsx_files = [f for f in files if f[-4:] == 'xlsx' and f[0] != '~' and f[0:4] != '2018']\n",
    "\n",
    "# Putting all the contents into dataframe for viewing and accessing\n",
    "df = pd.DataFrame()\n",
    "for f in xlsx_files:\n",
    "    data = pd.read_excel(f, 'FEguide')\n",
    "    df = df.append(data)\n",
    "    \n",
    "train_row_count = df.shape[0] # record the number of training data\n",
    "\n",
    "# appending data of 2018\n",
    "xlsx_test = [f for f in files if f[-4:] == 'xlsx' and f[0] != '~' and f[0:4] == '2018']\n",
    "test_data = pd.read_excel(xlsx_test[0], 'FEguide')\n",
    "df = df.append(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We extract the target value\n",
    "y = df['Comb Unrd Adj FE - Conventional Fuel'].as_matrix()\n",
    "\n",
    "# Examine if the target contains any empty or nan value\n",
    "# If it does, we need to drop the row without a valid target value\n",
    "# Fortunately, we don't find such row\n",
    "for i in y:\n",
    "    if isinstance(i,float) and math.isnan(i):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_features = list()\n",
    "\n",
    "\n",
    "# Professor had named a few features that we cannot use, we extract the name of those features\n",
    "da = np.array(df.columns)\n",
    "drop_index = list()\n",
    "for i in range(len(da)):\n",
    "    if \"FE\" in da[i]:\n",
    "        drop_index.append(da[i])\n",
    "    elif \"MPG\" in da[i]:\n",
    "        drop_index.append(da[i])\n",
    "    elif \"CO2\" in da[i]:\n",
    "        drop_index.append(da[i])\n",
    "    elif \"Smog\" in da[i]:\n",
    "        drop_index.append(da[i])\n",
    "    elif \"Guzzler\" in da[i]:\n",
    "        drop_index.append(da[i])\n",
    "    elif \"EPA\" in da[i]:\n",
    "        drop_index.append(da[i])\n",
    "    elif \"GHG\" in da[i]:\n",
    "        drop_index.append(da[i])\n",
    "\n",
    "dropout_features.extend(drop_index)\n",
    "df_1 = df.drop(drop_index, axis=1) ##########first drop : drop disallowed features##########\n",
    "############################################################################################\n",
    "\n",
    "# We examine each feature and count how many nan values it have\n",
    "# Save results in a dictionary\n",
    "\n",
    "column_names = df_1.columns.values\n",
    "nan_dict = {}\n",
    "\n",
    "for column_name in column_names:\n",
    "    count = 0\n",
    "    for i in range(0,df_1.shape[0]):\n",
    "        if isinstance(df_1.iloc[i][column_name],float) and math.isnan(df_1.iloc[i][column_name]):\n",
    "            count += 1\n",
    "    nan_dict[column_name] = count\n",
    "    \n",
    "drop_2 = list()\n",
    "for key in nan_dict:\n",
    "    if nan_dict[key]>=2000:\n",
    "        drop_2.append(key)\n",
    "\n",
    "dropout_features.extend(drop_2)\n",
    "df_2 = df_1.drop(drop_2, axis=1) ##########second drop : drop features without enough valid entry#########\n",
    "##########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Ethanol % - Gasoline: 107\n",
      "Descriptor - Model Type (40 Char or less): 1391\n",
      "Car/Truck Category - Cash for Clunkers Bill.: 1690\n",
      "Unique Label?: 420\n",
      "Label Recalc?: 248\n",
      "Var Valve Timing Desc: 151\n",
      "$ You Spend over 5 years (increased amount spent in fuel costs over 5 years - on label) : 1301\n"
     ]
    }
   ],
   "source": [
    "# After the second drop, we left with 48 features\n",
    "# We found 8 features have at least 1 nan values  \n",
    "# We have to handle that \n",
    "count_dict = {}\n",
    "column_names = df_2.columns.values\n",
    "for column_name in column_names:\n",
    "    count = 0\n",
    "    for i in range(0,df_2.shape[0]):\n",
    "        if isinstance(df_2.iloc[i][column_name],float) and math.isnan(df_2.iloc[i][column_name]):\n",
    "            count += 1\n",
    "    count_dict[column_name] = count\n",
    "    \n",
    "        \n",
    "for key in count_dict:\n",
    "    if count_dict[key] > 0:\n",
    "        print(str(key) + \": \" + str(count_dict[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_features.append(\"Car/Truck Category - Cash for Clunkers Bill.\")\n",
    "df_3 = df_2.drop(\"Car/Truck Category - Cash for Clunkers Bill.\", axis=1) #### third drop ####\n",
    "####### Car/Truck features is useless after examining it ####################################\n",
    "\n",
    "dropout_features.append(\"Release Date\") ###### Fourth drop ##################################\n",
    "df_4 = df_3.drop(\"Release Date\", axis=1) ##### We temporarily drop the date here ############\n",
    "\n",
    "\n",
    "dropout_features.append(\"Descriptor - Model Type (40 Char or less)\")  ######## fifth drop ###\n",
    "df_5 = df_4.drop(\"Descriptor - Model Type (40 Char or less)\", axis=1) #######################\n",
    "\n",
    "#################### We have done all dropping features by this point #######################\n",
    "#################### All dropout features are stored in dropout_features ####################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################ We fill out those four features manually ###################\n",
    "index_ = df_5.columns\n",
    "adjust_index = list()\n",
    "adjust_index.append(list(index_).index(\"Max Ethanol % - Gasoline\"))\n",
    "adjust_index.append(list(index_).index(\"Unique Label?\"))\n",
    "adjust_index.append(list(index_).index(\"Label Recalc?\"))\n",
    "adjust_index.append(list(index_).index(\"Var Valve Timing Desc\"))\n",
    "adjust_index\n",
    "\n",
    "for i in adjust_index:\n",
    "    for j in range(df_5.shape[0]):\n",
    "        if j != df_5.shape[0] - 1:\n",
    "            if isinstance(df_5.iat[j, i], float) and math.isnan(df_5.iat[j, i]):\n",
    "                if isinstance(df_5.iat[j + 1, i], float) and math.isnan(df_5.iat[j + 1, i]):\n",
    "                    df_5.iat[j, i] = df_5.iat[j - 1, i]\n",
    "                else:\n",
    "                    m = random.randint(0,1)\n",
    "                    if m == 0:\n",
    "                        df_5.iat[j, i] = df_5.iat[j - 1, i]\n",
    "                    else:\n",
    "                        df_5.iat[j, i] = df_5.iat[j + 1, i]\n",
    "        else:\n",
    "            if isinstance(df_5.iat[j, i],float) and math.isnan(df_5.iat[j, i]):\n",
    "                df_5.iat[j, i] = df_5.iat[j - 1, i]\n",
    "##############################################################################################\n",
    "\n",
    "############################# using get_dummies to fill out discrete features ################\n",
    "df_6 = pd.get_dummies(df_5)\n",
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import fancyimpute\n",
    "\n",
    "############################# using fancy_impute to impute data entries ######################\n",
    "X = df_6.as_matrix()\n",
    "mice = fancyimpute.MICE(verbose=0)\n",
    "X_fancy_mice = mice.complete(X)\n",
    "#############################################################################################\n",
    "\n",
    "# We assume the data are distributed i.i.d but we know we are going to use\n",
    "# 2018 data as test data\n",
    "X_15_17 = X_fancy_mice[0:train_row_count, :]\n",
    "X_18 = X_fancy_mice[train_row_count: , :]\n",
    "y_15_17 = y[0:train_row_count,]\n",
    "y_18 = y[train_row_count:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "# ridge_pipe = make_pipeline(StandardScaler(), RidgeCV())\n",
    "# scores = cross_val_score(ridge_pipe, X_fancy_mice, y, cv=10)\n",
    "# np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83219030286901319"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_pipe = make_pipeline(StandardScaler(), LassoCV())\n",
    "lasso_pipe.fit(X_15_17, y_15_17)\n",
    "lasso_pipe.score(X_18, y_18)\n",
    "# scores = cross_val_score(lasso_pipe, X_fancy_mice, y, cv=10)\n",
    "# np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_pipe_s = make_pipeline(StandardScaler(), Ridge())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'standardscaler', 'ridge', 'standardscaler__copy', 'standardscaler__with_mean', 'standardscaler__with_std', 'ridge__alpha', 'ridge__copy_X', 'ridge__fit_intercept', 'ridge__max_iter', 'ridge__normalize', 'ridge__random_state', 'ridge__solver', 'ridge__tol'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_pipe_s.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_15_17, y_15_17, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ridge__alpha': array([  1.00000000e-03,   1.00000000e-02,   1.00000000e-01,\n",
      "         1.00000000e+00,   1.00000000e+01,   1.00000000e+02,\n",
      "         1.00000000e+03])}\n",
      "{'ridge__alpha': 10.0}\n",
      "0.937839446773\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'ridge__alpha': np.logspace(-3, 3, 7)}\n",
    "print(param_grid)\n",
    "grid = GridSearchCV(ridge_pipe_s, param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean_fit_time', 'mean_score_time', 'mean_test_score',\n",
       "       'mean_train_score', 'param_ridge__alpha', 'params', 'rank_test_score',\n",
       "       'split0_test_score', 'split0_train_score', 'split1_test_score',\n",
       "       'split1_train_score', 'split2_test_score', 'split2_train_score',\n",
       "       'split3_test_score', 'split3_train_score', 'split4_test_score',\n",
       "       'split4_train_score', 'split5_test_score', 'split5_train_score',\n",
       "       'split6_test_score', 'split6_train_score', 'split7_test_score',\n",
       "       'split7_train_score', 'split8_test_score', 'split8_train_score',\n",
       "       'split9_test_score', 'split9_train_score', 'std_fit_time',\n",
       "       'std_score_time', 'std_test_score', 'std_train_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid.cv_results_).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_pipe_s_t = make_pipeline(StandardScaler(), Ridge(alpha=10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88917326875115887"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_pipe_s_t.fit(X_15_17, y_15_17)\n",
    "ridge_pipe_s_t.score(X_18, y_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ridge__alpha': array([  1.00000000e-03,   1.00000000e-02,   1.00000000e-01,\n",
      "         1.00000000e+00,   1.00000000e+01,   1.00000000e+02,\n",
      "         1.00000000e+03])}\n",
      "{'ridge__alpha': 0.10000000000000001}\n",
      "0.935385347254\n"
     ]
    }
   ],
   "source": [
    "ridge_pipe_s_mas = make_pipeline(MaxAbsScaler(), Ridge())\n",
    "param_grid_mas = {'ridge__alpha': np.logspace(-3, 3, 7)}\n",
    "print(param_grid_mas)\n",
    "grid_mas = GridSearchCV(ridge_pipe_s_mas, param_grid_mas, cv=10)\n",
    "grid_mas.fit(X_train, y_train)\n",
    "print(grid_mas.best_params_)\n",
    "print(grid_mas.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_pipe_s_t_mas = make_pipeline(MaxAbsScaler(), Ridge(alpha=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95984378109913415"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_pipe_s_t_mas.fit(X_15_17, y_15_17)\n",
    "ridge_pipe_s_t_mas.score(X_18, y_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fe = ridge_pipe_s_t_mas.steps[1][1].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(fe > 5)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  29,   47,   61,   87,   94,   99,  101,  110,  111,  112,  115,\n",
       "        116,  120,  160,  165,  166,  171,  172,  214,  238,  239,  241,\n",
       "        246,  252,  307,  314,  316,  317,  318,  343,  345,  349,  360,\n",
       "        362,  374,  375,  379,  380,  388,  389,  391,  392,  393,  395,\n",
       "        404,  405,  427,  432,  436,  437,  438,  441,  442,  444,  445,\n",
       "        454,  460,  469,  496,  500,  503,  505,  522,  536,  537,  538,\n",
       "        539,  542,  586,  587,  604,  620,  621,  622,  635,  637,  642,\n",
       "        649,  667,  668,  670,  674,  680,  693,  699,  715,  718,  729,\n",
       "        742,  745,  748,  760,  761,  766,  767,  773,  810,  832,  836,\n",
       "        845,  849,  852,  853,  857,  867,  873,  875,  876,  881,  896,\n",
       "        899,  900,  902,  904,  906,  914,  936,  937,  946,  947,  953,\n",
       "        954,  958,  959,  960,  961,  967,  972,  973,  974,  975, 1001,\n",
       "       1005, 1006, 1013, 1014, 1028, 1042, 1044, 1048, 1059, 1070, 1071,\n",
       "       1079, 1080, 1081, 1082, 1093, 1094, 1096, 1098, 1099, 1106, 1111,\n",
       "       1113, 1154, 1168, 1171, 1178, 1179, 1194, 1195, 1197, 1198, 1199,\n",
       "       1200, 1201, 1202, 1229, 1231, 1232, 1248, 1249, 1250, 1252, 1264,\n",
       "       1266, 1277, 1278, 1295, 1296, 1298, 1348, 1349, 1358, 1362, 1366,\n",
       "       1368, 1390, 1409, 1417, 1455, 1458, 1477, 1478, 1479, 1480, 1481,\n",
       "       1485, 1496, 1497, 1524, 1525, 1532, 1557, 1563, 1569, 1573, 1579,\n",
       "       1588, 1590, 1591, 1592, 1593, 1601, 1602, 1618, 1624, 1626, 1634,\n",
       "       1655, 1656, 1659, 1660, 1667, 1674, 1680, 1681, 1706, 1720, 1724,\n",
       "       1732, 1740, 1741, 1746, 1752])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = np.where(fe > 1)[0]\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3701, 236)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_15_17_sel = X_15_17[:, ind]\n",
    "X_15_17_sel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1227, 236)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_18_sel = X_18[:, ind]\n",
    "X_18_sel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_sel, X_val_sel, y_train_sel, y_val_sel = train_test_split(X_15_17_sel, X_15_17_sel, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "ridge_pipe_s_poly = make_pipeline(MaxAbsScaler(), PolynomialFeatures(interaction_only=True, include_bias=False), Ridge())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'maxabsscaler', 'polynomialfeatures', 'ridge', 'maxabsscaler__copy', 'polynomialfeatures__degree', 'polynomialfeatures__include_bias', 'polynomialfeatures__interaction_only', 'ridge__alpha', 'ridge__copy_X', 'ridge__fit_intercept', 'ridge__max_iter', 'ridge__normalize', 'ridge__random_state', 'ridge__solver', 'ridge__tol'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_pipe_s_poly.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ridge__alpha': array([  1.00000000e-03,   1.00000000e-02,   1.00000000e-01,\n",
      "         1.00000000e+00,   1.00000000e+01,   1.00000000e+02,\n",
      "         1.00000000e+03])}\n",
      "{'ridge__alpha': 0.001}\n",
      "0.979112507561\n"
     ]
    }
   ],
   "source": [
    "param_grid_poly = {'ridge__alpha': np.logspace(-3, 3, 7)}\n",
    "print(param_grid_poly)\n",
    "grid_poly = GridSearchCV(ridge_pipe_s_poly, param_grid_poly, cv=10)\n",
    "grid_poly.fit(X_train_sel, y_train_sel)\n",
    "print(grid_poly.best_params_)\n",
    "print(grid_poly.score(X_val_sel, y_val_sel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_pipe_s_poly_t = make_pipeline(MaxAbsScaler(), PolynomialFeatures(interaction_only=True, include_bias=False), Ridge(alpha=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72916991476298265"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_pipe_s_poly_t.fit(X_15_17_sel, y_15_17)\n",
    "ridge_pipe_s_poly_t.score(X_18_sel, y_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_pipe_poly = make_pipeline(MaxAbsScaler(), PolynomialFeatures(interaction_only=True, include_bias=False), Ridge())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ridge__alpha': array([  1.00000000e-03,   1.00000000e-02,   1.00000000e-01,\n",
      "         1.00000000e+00,   1.00000000e+01,   1.00000000e+02,\n",
      "         1.00000000e+03])}\n"
     ]
    }
   ],
   "source": [
    "param_grid_poly = {'ridge__alpha': np.logspace(-3, 3, 7)}\n",
    "print(param_grid_poly)\n",
    "grid_poly_f = GridSearchCV(ridge_pipe_poly, param_grid_poly, cv=10)\n",
    "grid_poly_f.fit(X_train, y_train)\n",
    "print(grid_poly_f.best_params_)\n",
    "print(grid_poly_f.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ridge_pipe_poly_t = make_pipeline(MaxAbsScaler(), PolynomialFeatures(interaction_only=True, include_bias=False), Ridge(alpha=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_pipe_poly_t.fit(X_15_17, y_15_17)\n",
    "# ridge_pipe_poly_t.score(X_18, y_18)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
